# Example values.yaml for OpenAD Model Helm Chart
# This file demonstrates common configurations and their explanations

# Number of pod replicas to run
replicaCount: 1

# BuildConfig settings for OpenShift
buildConfig:
  name: example-model-build  # Name of the BuildConfig
  gitUri: "https://github.com/username/repo"  # Git repository URL
  gitRef: "main"  # Git branch, tag, or commit
  strategy: Docker  # Build strategy (Docker or Source)
  dockerfilePath: Dockerfile  # Path to Dockerfile in repo
  sourceSecret: {}  # Git credentials if needed

# Container image configuration
image:
  repository: example-model-service  # Image name
  tag: "latest"  # Image tag
  pullPolicy: IfNotPresent  # Image pull policy
  env:
    # HuggingFace cache directory
    - HF_HOME: "/tmp/.cache/huggingface"
    # Matplotlib config directory
    - MPLCONFIGDIR: "/tmp/.config/matplotlib"
    # Application logging path
    - LOGGING_CONFIG_PATH: "/tmp/app.log"
    # OpenAD models cache path - mount checkpoints here
    - gt4sd_local_cache_path: "/data/.openad_models"
    # Cache for inference results (only for deterministic models)
    - ENABLE_CACHE_RESULTS: "False"

# Override the release name
nameOverride: ""
# Override the full release name
fullnameOverride: ""

# ServiceAccount configuration
serviceAccount:
  create: false  # Whether to create a ServiceAccount
  automount: true  # Auto-mount API credentials
  annotations: {}  # ServiceAccount annotations
  name: ""  # ServiceAccount name (generated if not set)

# Pod annotations and labels
podAnnotations: {}
podLabels: {}

# Pod security context
podSecurityContext: {}
  # fsGroup: 2000

# Container security context
securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

# Service configuration
service:
  type: ClusterIP  # Service type (ClusterIP, NodePort, LoadBalancer)
  port: 80  # External port
  targetPort: 8080  # Container port

# Ingress configuration
ingress:
  enabled: false  # Enable ingress
  className: ""  # Ingress class
  annotations: {}  # Ingress annotations
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: model-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []  # TLS configuration
    # - secretName: model-example-tls
    #   hosts:
    #     - model-example.local

# Resource requests and limits
resources:
  limits:
    # nvidia.com/gpu: 1  # Uncomment for GPU support
    cpu: 10000m  # 10 CPU cores
    memory: "10Gi"  # 10GB memory
  requests:
    # nvidia.com/gpu: 1  # Uncomment for GPU support
    cpu: 1000m  # 1 CPU core
    memory: "3Gi"  # 3GB memory

# Health check configuration
livenessProbe:
  httpGet:
    path: /health
    port: 8081
  initialDelaySeconds: 10
  periodSeconds: 15
  timeoutSeconds: 10
  successThreshold: 1
  failureThreshold: 3

readinessProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 10
  periodSeconds: 15
  timeoutSeconds: 10
  successThreshold: 1
  failureThreshold: 5

# Horizontal Pod Autoscaling
autoscaling:
  enabled: true  # Enable HPA
  minReplicas: 1  # Minimum replicas
  maxReplicas: 2  # Maximum replicas
  targetCPUUtilizationPercentage: 80  # CPU target
  targetMemoryUtilizationPercentage: 80  # Memory target

# Volume configuration
volumes:
  - name: s3-data-pvc  # PVC for model data

volumeMounts:
  - name: s3-data-pvc  # Mount the PVC
    mountPath: "/data"  # Mount path in container

# AWS configuration for S3 data download
aws: {}  # AWS credentials and config

# Node selection
nodeSelector: {}

# Pod tolerations
tolerations: []
  # Uncomment for GPU nodes
  # - key: "nvidia.com/gpu"
  #   effect: "NoSchedule"

# Pod affinity rules
affinity: {}
